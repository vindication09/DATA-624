---
title: "DATA 624 Week 6"
author: "Vinicio Haro"
date: "October 9, 2019"
output: html_document
---

#Week 6 HW

#HA  8.1, 8.2, 8.6, 8.8

## 8.1
Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers

* a) Explain the differences among these figures. Do they all indicate that the data are white noise?


White noise can be identified by seeing if the correlations fall between the dotted lines. In this case, across all three figures, we see this occuring. We should also keep in mind that the graphs are showing the correlations among the different series types. We see correlations for differing lag periods, which in our case seem to increase. It should also be noted that the area between the dotted lines indicates significant critical values for ACF. 

* b) Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?

We know that as sample size n increases, then critical value bounded region tends to become smaller. We should also keep in mind that the critical values are computed as follows: 

$$
+\frac{1.96}{\sqrt(N)}\\
or\\
-\frac{1.96}{\sqrt(N)}
$$

## 8.2
classic example of a non-stationary series is the daily closing IBM stock price series (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.

```{r warning=FALSE, message=FALSE}
library(fpp2)

ggtsdisplay(ibmclose)
```


The first thing that comes to mind is the ACF plot. We can see that autocorrelations are well above the critical value space, but are decreasing steadily. We can also observe evidence of a trend in addition to seeing that the stock data is non-stationary. 

The PACF plot reveals that the data could potentially be predicted using single lag units. Overall, from what we gathered, we see there is significant autocorrelation. We should impliment data differences to improve overall stability of time series mean before any sort of forecast/predictions are done. 


## 8.6
Use R to simulate and plot some data from simple ARIMA models.

* a) Use the following R code to generate data from an AR(1) model (Take parameters given in text)

```{r warning=FALSE, message=FALSE}
y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
  y[i] <- 0.6*y[i-1] + e[i]
```

* b) Produce a time plot for the series. How does the plot change as you change phi ?

```{r warning=FALSE, message=FALSE}

ar1 <- function(phi, sd=1, n=100)
  {
  y <- ts(numeric(n))
  
  e <- rnorm(n, sd=sd)
  
  for(i in 2:n)
    
    y[i] <- phi*y[i-1] + e[i]
  
  return(y)
  }
```


```{r warning=FALSE, message=FALSE}
data <- list() #collect data 

i <- 0 #initialize

phi_values <- c(0.000001 ,0.0001, 0.001, 0.1) * 6 #create a vector that tries several phi values

for (phi in phi_values)
  {
  i <- i + 1
  data[[i]] <- ar1(phi) #run against AR1
  }

data2 <- do.call(cbind, data)

colnames(data2) <- paste('phi=', phi_values, sep = '')

autoplot(data2) + ylab('Data')

```

It is difficult to see how exactly each value of phi alters the series. Lets break it up. 

```{r warning=FALSE, message=FALSE}
d1 <- list() #collect data 
d2 <- list() #collect data 
d3 <- list() #collect data 
d4 <- list() #collect data 


i <- 0 #initialize

phi1 <- c(0.000001)*6
phi2 <- c(0.0001)*6
phi3 <- c(0.001)*6
phi4 <- c(0.1)*6 

for (phi in phi1)
  {
  i <- i + 1
  d1[[i]] <- ar1(phi) #run against AR1
}

for (phi in phi2)
  {
  i <- i + 1
  d2[[i]] <- ar1(phi) #run against AR1
}

for (phi in phi3)
  {
  i <- i + 1
  d3[[i]] <- ar1(phi) #run against AR1
}

for (phi in phi4)
  {
  i <- i + 1
  d4[[i]] <- ar1(phi) #run against AR1
  }


data21 <- do.call(cbind, d1)
data22 <- do.call(cbind, d2)
data23 <- do.call(cbind, d3)
data24 <- do.call(cbind, d4)

#colnames(data21) <- paste('phi=', phi1, sep = '')
#colnames(data22) <- paste('phi=', phi2, sep = '')
#colnames(data23) <- paste('phi=', phi3, sep = '')
#colnames(data24) <- paste('phi=', phi4, sep = '')

autoplot(data21) + ylab('Data')+ggtitle("Phi 1")
autoplot(data22) + ylab('Data')+ggtitle("Phi 2");
autoplot(data23) + ylab('Data')+ ggtitle("Phi 3");
autoplot(data24) + ylab('Data')+ ggtitle("Phi 4")
```

Phi1 is our smallest of phi values while phi4 is the largest of our phi values. When phi gets larger, we see much more variation when it comes to our y values. We should check some additional plots,specifically the ACF plots. 

```{r warning=FALSE, message=FALSE}
par(mfrow=c(1,3))
acf(data21, main='Phi1');
acf(data22, main='Phi2');
acf(data23, main='Phi3');
acf(data24, main='Phi4')
```

The ACF plots are much more revealing especially when we look at the ACF for our largest phi value. We clearly see the autocorrelations extending beyond the space where correlations are considered to be significant. 

* c) Write your own code to generate data from an MA(1) model 
 


